{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let first connect to the driver\n",
    "\n",
    "driver=webdriver.Edge(r\"msedgedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening page on automated Edge browser\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guitar\n"
     ]
    }
   ],
   "source": [
    "# Now lets give the input that we wnat to search\n",
    "# Lets take guitar only\n",
    "\n",
    "input1=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets go the search box \n",
    "\n",
    "search_box=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_box.send_keys(input1)\n",
    "# and enter input1=Guitar as a input inside the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button_click=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_button_click.click()\n",
    "# clicking on the seach button over here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all product urls\n",
    "product_urls = []\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 3 page\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class = \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')       #scraping urls\n",
    "    for i in url:\n",
    "        product_urls.append(i.get_attribute(\"href\"))                        #appending the urls in product_urls list\n",
    "    nxt_button=driver.find_element(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')     #scraping the list of buttons from the page\n",
    "    nxt_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank list variables   \n",
    "Brand=[]\n",
    "product = []\n",
    "price = []\n",
    "availability=[]\n",
    "ReturnExchange=[]\n",
    "Delivery=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in product_urls[:10]: ##loop for every guitar in the list\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        brand= driver.find_element(By.XPATH,'//*[@id=\"productOverview_feature_div\"]/div/table/tbody/tr[1]/td[2]/span')\n",
    "        Brand.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-') \n",
    "        \n",
    "    try:\n",
    "        exchange = driver.find_element(By.XPATH,'//*[@id=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        ReturnExchange.append(exchange.text)\n",
    "    except NoSuchElementException:\n",
    "        ReturnExchange.append('-') \n",
    "    try:\n",
    "        delivery= driver.find_element(By.XPATH,'//span[@class=\"a-text-bold\"]')\n",
    "        Delivery.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        Delivery.append('-') \n",
    "        \n",
    "    \n",
    "    try:\n",
    "        product.append(driver.find_element(By.XPATH,'//span[@id=\"productTitle\"]').text)\n",
    "    except:\n",
    "        product.append('-')\n",
    "    try:\n",
    "        try:\n",
    "            price.append(driver.find_element(By.XPATH,'//td[@class=\"a-span12\"]').text)\n",
    "        except:\n",
    "            price.append(driver.find_element_by_css_selector('span[class=\"a-price-whole\"]').text)             \n",
    "    except:\n",
    "        price.append('-')\n",
    "    try:\n",
    "        availability.append( driver.find_element(By.XPATH,'//div[@id=\"availability\"]').text)\n",
    "    except:\n",
    "        availability.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),len(ReturnExchange),len(Delivery),len(product),len(price),len(availability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>ReturnExchange</th>\n",
       "      <th>Price</th>\n",
       "      <th>Delievery_by</th>\n",
       "      <th>urls</th>\n",
       "      <th>Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>-</td>\n",
       "      <td>Monday, September 5</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>In stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FESTRA</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>-</td>\n",
       "      <td>Wednesday, September 7</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>In stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown</td>\n",
       "      <td>7 Days Returnable</td>\n",
       "      <td>-</td>\n",
       "      <td>Thursday, September 8</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>In stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gum-Tea</td>\n",
       "      <td>Non-Returnable</td>\n",
       "      <td>-</td>\n",
       "      <td>September 9 - 12</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>Only 2 left in stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juârez</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>₹2,090.00</td>\n",
       "      <td>Sunday, September 4</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>In stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Intern</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>-</td>\n",
       "      <td>Wednesday, September 7</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intern</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>₹2,089.00</td>\n",
       "      <td>Wednesday, September 7</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>In stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Intern</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>-</td>\n",
       "      <td>Wednesday, September 7</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>₹4,999.00</td>\n",
       "      <td>Monday, September 5</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JUAREZ</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>-</td>\n",
       "      <td>Wednesday, September 7</td>\n",
       "      <td>https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...</td>\n",
       "      <td>In stock.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Brand      ReturnExchange      Price            Delievery_by  \\\n",
       "0  Kadence  7 Days Replacement          -     Monday, September 5   \n",
       "1   FESTRA  7 Days Replacement          -  Wednesday, September 7   \n",
       "2    Brown   7 Days Returnable          -   Thursday, September 8   \n",
       "3  Gum-Tea      Non-Returnable          -        September 9 - 12   \n",
       "4   Juârez  7 Days Replacement  ₹2,090.00     Sunday, September 4   \n",
       "5   Intern  7 Days Replacement          -  Wednesday, September 7   \n",
       "6   Intern  7 Days Replacement  ₹2,089.00  Wednesday, September 7   \n",
       "7   Intern  7 Days Replacement          -  Wednesday, September 7   \n",
       "8  Kadence  7 Days Replacement  ₹4,999.00     Monday, September 5   \n",
       "9   JUAREZ  7 Days Replacement          -  Wednesday, September 7   \n",
       "\n",
       "                                                urls           Availability  \n",
       "0  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...              In stock.  \n",
       "1  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...              In stock.  \n",
       "2  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...              In stock.  \n",
       "3  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...  Only 2 left in stock.  \n",
       "4  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...              In stock.  \n",
       "5  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...  Only 1 left in stock.  \n",
       "6  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...              In stock.  \n",
       "7  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...  Only 1 left in stock.  \n",
       "8  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...                         \n",
       "9  https://www.amazon.in/JUAREZ-JRZ38C-Acoustic-S...              In stock.  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Brand':Brand,'ReturnExchange':ReturnExchange,'Price':price,'Delievery_by':Delivery,'urls':url,'Availability':availability})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let first connect to the driver\n",
    "\n",
    "driver=webdriver.Edge(r\"msedgedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining 0 of 300 images\n",
      "Obtaining 1 of 300 images\n",
      "Obtaining 2 of 300 images\n",
      "Obtaining 3 of 300 images\n",
      "Obtaining 4 of 300 images\n",
      "Obtaining 5 of 300 images\n",
      "Obtaining 6 of 300 images\n",
      "Obtaining 7 of 300 images\n",
      "Obtaining 8 of 300 images\n",
      "Obtaining 9 of 300 images\n",
      "Obtaining 10 of 300 images\n",
      "Obtaining 11 of 300 images\n",
      "Obtaining 12 of 300 images\n",
      "Obtaining 13 of 300 images\n",
      "Obtaining 14 of 300 images\n",
      "Obtaining 15 of 300 images\n",
      "Obtaining 16 of 300 images\n",
      "Obtaining 17 of 300 images\n",
      "Obtaining 18 of 300 images\n",
      "Obtaining 19 of 300 images\n",
      "Obtaining 20 of 300 images\n",
      "Obtaining 21 of 300 images\n",
      "Obtaining 22 of 300 images\n",
      "Obtaining 23 of 300 images\n",
      "Obtaining 24 of 300 images\n",
      "Obtaining 25 of 300 images\n",
      "Obtaining 26 of 300 images\n",
      "Obtaining 27 of 300 images\n",
      "Obtaining 28 of 300 images\n",
      "Obtaining 29 of 300 images\n",
      "Obtaining 30 of 300 images\n",
      "Obtaining 31 of 300 images\n",
      "Obtaining 32 of 300 images\n",
      "Obtaining 33 of 300 images\n",
      "Obtaining 34 of 300 images\n",
      "Obtaining 35 of 300 images\n",
      "Obtaining 36 of 300 images\n",
      "Obtaining 37 of 300 images\n",
      "Obtaining 38 of 300 images\n",
      "Obtaining 39 of 300 images\n",
      "Obtaining 40 of 300 images\n",
      "Obtaining 41 of 300 images\n",
      "Obtaining 42 of 300 images\n",
      "Obtaining 43 of 300 images\n",
      "Obtaining 44 of 300 images\n",
      "Obtaining 45 of 300 images\n",
      "Obtaining 46 of 300 images\n",
      "Obtaining 47 of 300 images\n",
      "Obtaining 48 of 300 images\n",
      "Obtaining 49 of 300 images\n",
      "Obtaining 50 of 300 images\n",
      "Obtaining 51 of 300 images\n",
      "Obtaining 52 of 300 images\n",
      "Obtaining 53 of 300 images\n",
      "Obtaining 54 of 300 images\n",
      "Obtaining 55 of 300 images\n",
      "Obtaining 56 of 300 images\n",
      "Obtaining 57 of 300 images\n",
      "Obtaining 58 of 300 images\n",
      "Obtaining 59 of 300 images\n",
      "Obtaining 60 of 300 images\n",
      "Obtaining 61 of 300 images\n",
      "Obtaining 62 of 300 images\n",
      "Obtaining 63 of 300 images\n",
      "Obtaining 64 of 300 images\n",
      "Obtaining 65 of 300 images\n",
      "Obtaining 66 of 300 images\n",
      "Obtaining 67 of 300 images\n",
      "Obtaining 68 of 300 images\n",
      "Obtaining 69 of 300 images\n",
      "Obtaining 70 of 300 images\n",
      "Obtaining 71 of 300 images\n",
      "Obtaining 72 of 300 images\n",
      "Obtaining 73 of 300 images\n",
      "Obtaining 74 of 300 images\n",
      "Obtaining 75 of 300 images\n",
      "Obtaining 76 of 300 images\n",
      "Obtaining 77 of 300 images\n",
      "Obtaining 78 of 300 images\n",
      "Obtaining 79 of 300 images\n",
      "Obtaining 80 of 300 images\n",
      "Obtaining 81 of 300 images\n",
      "Obtaining 82 of 300 images\n",
      "Obtaining 83 of 300 images\n",
      "Obtaining 84 of 300 images\n",
      "Obtaining 85 of 300 images\n",
      "Obtaining 86 of 300 images\n",
      "Obtaining 87 of 300 images\n",
      "Obtaining 88 of 300 images\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='encrypted-tbn0.gstatic.com', port=443): Max retries exceeded with url: /images?q=tbn:ANd9GcSXmC8c56cSGqmf4nmxEapqYh6RhMT_iqxJnA&usqp=CAU (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000019B657A5E80>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000019B657A5E80>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='encrypted-tbn0.gstatic.com', port=443): Max retries exceeded with url: /images?q=tbn:ANd9GcSXmC8c56cSGqmf4nmxEapqYh6RhMT_iqxJnA&usqp=CAU (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000019B657A5E80>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-3e897cb6493e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Obtaining {0} of {1} images\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"D\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='encrypted-tbn0.gstatic.com', port=443): Max retries exceeded with url: /images?q=tbn:ANd9GcSXmC8c56cSGqmf4nmxEapqYh6RhMT_iqxJnA&usqp=CAU (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000019B657A5E80>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))"
     ]
    }
   ],
   "source": [
    "\n",
    "url='https://images.google.com/'\n",
    "urls = []    # empty list for image urls to download\n",
    "data = []    # empty list for storing data\n",
    "search_item = [\"Fruits\", \"Cars\", \"Machine Learning\"]\n",
    "for item in search_item:\n",
    "    driver.get(url)  # performing a Google search for our selected items\n",
    "    time.sleep(3)\n",
    "    search_bar = driver.find_element(By.TAG_NAME,\"input\")     # finding search bar on Google page   \n",
    "    search_bar.send_keys(str(item))      # send search item list    \n",
    "    search_button =driver.find_element(By.XPATH,\"//button[@class='Tg7LZd']\").click() # now we will click on search button\n",
    "    \n",
    "    # scroll the page in order to generate more images on the website\n",
    "    for _ in range(500):\n",
    "        driver.execute_script(\"window.scrollBy(0,100)\")        \n",
    "        imgs = driver.find_elements(By.XPATH,\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "                if(source[0:4] == 'http'):\n",
    "                    img_url.append(source)\n",
    "    for i in img_url[:100]:      # getting only 100 images each\n",
    "        urls.append(i)\n",
    "                    \n",
    "for i in range(len(urls)):\n",
    "    if i >= 300:\n",
    "        break\n",
    "    print(\"Obtaining {0} of {1} images\".format(i, 300))\n",
    "    response=requests.get(urls[i])\n",
    "    file = open(r\"C:\\Users\\Ali khan\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There was connection error otherwise its working fine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let first connect to the driver\n",
    "\n",
    "driver=webdriver.Edge(r\"msedgedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "\n",
    "# opning flipkart.com\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(3)\n",
    "try:\n",
    "    login_X_button = driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]')                      # Button to close login popup\n",
    "    login_X_button.click()\n",
    "except NoSuchElementException : \n",
    "    print(\"No Login page\")\n",
    "search_bar = driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')    # Finding the search bar using it's xpath\n",
    "search_bar.clear()               # Clearing the search bar\n",
    "search_bar.send_keys(\"pixel\")       # Inputing keyword to search\n",
    "search_button = driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')    # Finding the xpath of search button\n",
    "search_button.click()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching urls of phones coming on 1st page\n",
    "flip_urls = []\n",
    "urls = driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    flip_urls.append(url.get_attribute(\"href\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "#Make empty lists   \n",
    "flip_dict = {}\n",
    "flip_dict[\"Brand\"] = []\n",
    "flip_dict[\"Smartphone\"] = []\n",
    "flip_dict[\"Colour\"] = []\n",
    "flip_dict[\"RAM\"] = []\n",
    "flip_dict[\"Storage(ROM)\"] = []\n",
    "flip_dict[\"Primary Camera\"] = []\n",
    "flip_dict[\"Secondary Camera\"] = []\n",
    "flip_dict[\"Display Size\"] = []\n",
    "flip_dict[\"Display Resolution\"] = []\n",
    "flip_dict[\"Processor\"] = []\n",
    "flip_dict[\"Processor Cores\"] = []\n",
    "flip_dict[\"Battery Capacity\"] = []\n",
    "flip_dict[\"Battery Type\"] = []\n",
    "flip_dict[\"Price\"] = []\n",
    "flip_dict[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL =  https://www.flipkart.com/google-pixel-6a-chalk-128-gb/p/itme5ae89135d44e?pid=MOBGFWEZ5SKU84Z8&lid=LSTMOBGFWEZ5SKU84Z8FXPB45&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGFWEZ5SKU84Z8.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/google-pixel-6a-charcoal-128-gb/p/itme5ae89135d44e?pid=MOBGFKX5YUXD74Z3&lid=LSTMOBGFKX5YUXD74Z3MXA2OB&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_2&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGFKX5YUXD74Z3.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/google-pixel-4a-just-black-128-gb/p/itm023b9677aa45d?pid=MOBFUSBNAZGY7HQU&lid=LSTMOBFUSBNAZGY7HQUWHTF0C&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_3&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBFUSBNAZGY7HQU.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/google-pixel-3a-clearly-white-64-gb/p/itmfgk4jfgstaack?pid=MOBFFGFPJSCEXMSG&lid=LSTMOBFFGFPJSCEXMSGODGRZE&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_4&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBFFGFPJSCEXMSG.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/realme-c30-locked-airtel-prepaid/p/itm938b2a52e171f?pid=MOBGFCFQXDH5NZGU&lid=LSTMOBGFCFQXDH5NZGUMBWKGV&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_5&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGFCFQXDH5NZGU.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/vivo-t1-44w-ice-dawn-128-gb/p/itm2a08ebbea3689?pid=MOBGDRHVKYHCVG7W&lid=LSTMOBGDRHVKYHCVG7WU0WEW8&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_6&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDRHVKYHCVG7W.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/redmi-10a-slate-grey-32-gb/p/itm755830dd54ae1?pid=MOBGDRGQXGRHUJS2&lid=LSTMOBGDRGQXGRHUJS2UT74ZB&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_7&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDRGQXGRHUJS2.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/realme-c30-locked-airtel-prepaid/p/itm938b2a52e171f?pid=MOBGFCFQZGEUXH35&lid=LSTMOBGFCFQZGEUXH35331CCP&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_8&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGFCFQZGEUXH35.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/realme-c30-locked-airtel-prepaid/p/itm938b2a52e171f?pid=MOBGFCFQTSHGRPYA&lid=LSTMOBGFCFQTSHGRPYATV0I3W&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_9&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGFCFQTSHGRPYA.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/vivo-t1-44w-starry-sky-128-gb/p/itm2a08ebbea3689?pid=MOBGDRHVHNBBBBP5&lid=LSTMOBGDRHVHNBBBBP57SXRTY&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_10&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDRHVHNBBBBP5.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/vivo-t1-44w-midnight-galaxy-128-gb/p/itm2a08ebbea3689?pid=MOBGDRHVZN29ZJF4&lid=LSTMOBGDRHVZN29ZJF4QSKOSJ&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_11&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDRHVZN29ZJF4.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/vivo-t1-44w-starry-sky-128-gb/p/itm2a08ebbea3689?pid=MOBGDRHVMW2UDXZY&lid=LSTMOBGDRHVMW2UDXZYRBYUFR&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_12&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDRHVMW2UDXZY.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/infinix-smart-6-light-sea-green-64-gb/p/itmd4041ce33d262?pid=MOBGDC6HQJZKSX3K&lid=LSTMOBGDC6HQJZKSX3KUVADVY&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_13&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDC6HQJZKSX3K.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/redmi-9a-sport-coral-green-32-gb/p/itm53bc6ebfe147b?pid=MOBG7CJZZRCJ3XHG&lid=LSTMOBG7CJZZRCJ3XHGTOBYKE&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_14&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBG7CJZZRCJ3XHG.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/vivo-t1-44w-ice-dawn-128-gb/p/itm2a08ebbea3689?pid=MOBGDRHVZ2F9AAM8&lid=LSTMOBGDRHVZ2F9AAM8DRR9FD&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_15&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDRHVZ2F9AAM8.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/redmi-10-prime-bifrost-blue-64-gb/p/itm9377b4c936baa?pid=MOBG6RGHNFKZUPRE&lid=LSTMOBG6RGHNFKZUPREA4DKUN&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_16&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBG6RGHNFKZUPRE.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/redmi-note-10-pro-max-dark-nebula-128-gb/p/itm32dc939d5a40f?pid=MOBGF8VJY8ZFKPMK&lid=LSTMOBGF8VJY8ZFKPMKQSKZBG&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_17&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGF8VJY8ZFKPMK.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/redmi-9a-sport-metallic-blue-32-gb/p/itm53bc6ebfe147b?pid=MOBG7H8GFGZPYUFH&lid=LSTMOBG7H8GFGZPYUFHF7NHKK&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_18&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBG7H8GFGZPYUFH.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/redmi-9a-sport-carbon-black-32-gb/p/itm53bc6ebfe147b?pid=MOBG7CJMHFQJFCX3&lid=LSTMOBG7CJMHFQJFCX31WPOBO&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_19&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBG7CJMHFQJFCX3.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/infinix-smart-6-polar-black-64-gb/p/itmd4041ce33d262?pid=MOBGDC6HZHYVX96A&lid=LSTMOBGDC6HZHYVX96AHNLOZE&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_20&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDC6HZHYVX96A.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/infinix-smart-6-heart-ocean-64-gb/p/itmd4041ce33d262?pid=MOBGDC6HN5VTU6RR&lid=LSTMOBGDC6HN5VTU6RRUSMZV8&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_21&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDC6HN5VTU6RR.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Scraping URL =  https://www.flipkart.com/infinix-smart-6-starry-purple-64-gb/p/itmd4041ce33d262?pid=MOBGDC6HFGCW5PGF&lid=LSTMOBGDC6HFGCW5PGFFKFQIM&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_22&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGDC6HFGCW5PGF.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL =  https://www.flipkart.com/redmi-note-10-pro-max-dark-night-128-gb/p/itm3b34df1da19de?pid=MOBGB7255D2JWWKM&lid=LSTMOBGB7255D2JWWKMBHBQUP&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_23&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGB7255D2JWWKM.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n",
      "Exception Occured. Moving to next page\n",
      "Scraping URL =  https://www.flipkart.com/realme-c30-bamboo-green-32-gb/p/itmfcf69b2db5059?pid=MOBGF37GUWGZQYGR&lid=LSTMOBGF37GUWGZQYGRW97HI6&marketplace=FLIPKART&q=pixel&store=tyy%2F4io&srno=s_1_24&otracker=search&otracker1=search&fm=organic&iid=92342d1a-0956-427a-815f-72af6e326d42.MOBGF37GUWGZQYGR.SEARCH&ppt=hp&ppn=homepage&ssid=1dkkcwno8g0000001662122515884&qH=ab4086ecd47c568d\n"
     ]
    }
   ],
   "source": [
    "# Scraping data from each url\n",
    "for url in flip_urls:\n",
    "    driver.get(url)    # Saving url                                                     \n",
    "    print(\"Scraping URL = \", url)\n",
    "    flip_dict['URL'].append(url)                                                          # Loading the webpage by url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        read_more = driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]')     # Button for expanding the specs\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception Occured. Moving to next page\")\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')      # Extracting Brand from xpath\n",
    "        flip_dict[\"Brand\"].append(brand.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Brand'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')      # Extracting Price from xpath\n",
    "        flip_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Price'].append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')      # Extracting Name from xpath\n",
    "        flip_dict['Smartphone'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Smartphone'].append('-')\n",
    "        \n",
    "    try:\n",
    "        color = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      # Extracting colour from xpath\n",
    "        flip_dict['Colour'].append(color.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Colour'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  # Extracting Display Size from xpath\n",
    "        flip_dict['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Size'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_res = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting Display Resolution from xpath\n",
    "        flip_dict['Display Resolution'].append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Resolution'].append('-')\n",
    "    \n",
    "    try:\n",
    "        pro_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_chk.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   # Extracting processor from xpath\n",
    "        flip_dict['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor'].append('-')\n",
    "    \n",
    "    try:                                                                                     # Extracting Processor Cores from xpath\n",
    "        core_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_chk.text != \"Processor Core\" :\n",
    "            core_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_chk.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        flip_dict['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor Cores'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rom = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')         # Extracting Storage(ROM) from xpath\n",
    "        flip_dict['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Storage(ROM)'].append('-')\n",
    "    \n",
    "    try:\n",
    "        ram = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                # Extracting RAM from xpath\n",
    "        flip_dict['RAM'].append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['RAM'].append('-')\n",
    "    try:                                                                                  \n",
    "        pri_cam = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting Camera from xpath\n",
    "        flip_dict['Primary Camera'].append(pri_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Primary Camera'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Secondary Camera from xpath\n",
    "        cam_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if cam_chk != \"Secondary Camera\" : \n",
    "            if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        flip_dict['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Secondary Camera'].append('-')\n",
    "        \n",
    "    try:\n",
    "        if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :   # Extracting Battery Capacity from xpath\n",
    "            if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                \n",
    "            elif driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :     \n",
    "                bat_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_cap = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Capacity'].append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Capacity'].append('-')\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            elif driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[1]')\n",
    "            if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "            bat_typ = driver.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[2]/ul/li')             # Extracting Battery Type from xpath\n",
    "        flip_dict['Battery Type'].append(bat_typ.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Type'].append('-')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28 24 28 28\n"
     ]
    }
   ],
   "source": [
    "print(len(flip_dict[\"Brand\"]), len(flip_dict[\"Smartphone\"]), len(flip_dict[\"Processor\"]), len(flip_dict[\"Price\"]), len(flip_dict['URL']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Extracted:  https://www.google.co.in/maps/place/New+York,+NY,+USA/@40.6971494,-74.2598817,10z/data=!3m1!4b1!4m5!3m4!1s0x89c24fa5d33f083b:0xc80b8f06e177fe62!8m2!3d40.7127753!4d-74.0059728\n",
      "Latitude = 40.6971494, Longitude = -74.2598817\n"
     ]
    }
   ],
   "source": [
    "# Let first connect to the driver\n",
    "import re\n",
    "driver=webdriver.Edge(r\"msedgedriver.exe\")\n",
    "# opening google maps\n",
    "url = \"https://www.google.co.in/maps\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "search = driver.find_element(By.ID,\"searchboxinput\")                       # locating search bar\n",
    "search.clear()                                                             # clearing search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(\"New york\")                                                     # entering values in search bar\n",
    "button = driver.find_element(By.ID,\"searchbox-searchbutton\")               # locating search button\n",
    "button.click()                                                             # clicking search button\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup Name</th>\n",
       "      <th>Industry/Vertical</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>Location</th>\n",
       "      <th>Investor</th>\n",
       "      <th>Investment Type</th>\n",
       "      <th>Amount(in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>M&amp;A</td>\n",
       "      <td>1,200,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Vedantu</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Coatue Management</td>\n",
       "      <td>Series D</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Crio</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Learning Platform for Developers</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>021 Capital</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>934,160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>goDutch</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Group Payments</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Matrix India,Y Combinator, Global Founders Cap...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Mystifly</td>\n",
       "      <td>Airfare Marketplace</td>\n",
       "      <td>Ticketing, Airline Retailing, and Post-Ticketi...</td>\n",
       "      <td>Singapore and Bangalore</td>\n",
       "      <td>Recruit Co. Ltd.</td>\n",
       "      <td>pre-Series B</td>\n",
       "      <td>3,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>JetSynthesys</td>\n",
       "      <td>Gaming and Entertainment</td>\n",
       "      <td>Gaming and Entertainment</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Adar Poonawalla and Kris Gopalakrishnan.</td>\n",
       "      <td>Venture-Series Unknown</td>\n",
       "      <td>400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10/07/2020</td>\n",
       "      <td>gigIndia</td>\n",
       "      <td>Marketplace</td>\n",
       "      <td>Crowd Sourcing, Freelance</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Incubate Fund India and Beyond Next Ventures</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>974,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>PumPumPum</td>\n",
       "      <td>Automotive Rental</td>\n",
       "      <td>Used Car-leasing platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Early Adapters Syndicate</td>\n",
       "      <td>Seed</td>\n",
       "      <td>292,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>FLYX</td>\n",
       "      <td>OTT Player</td>\n",
       "      <td>Streaming Social Network</td>\n",
       "      <td>New York and Delhi</td>\n",
       "      <td>Raj Mishra, founder of AIT Global Inc</td>\n",
       "      <td>pre-Seed</td>\n",
       "      <td>200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Open Appliances Pvt. Ltd.</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Internet-of-Things Security Solutions</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Unicorn India Ventures</td>\n",
       "      <td>Venture-Series Unknown</td>\n",
       "      <td>500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15/08/2020</td>\n",
       "      <td>Practo</td>\n",
       "      <td>HealthTech</td>\n",
       "      <td>Health care and Wellness</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>A1A Company</td>\n",
       "      <td>Series F</td>\n",
       "      <td>32,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>Medlife</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Pharmacy</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Prasid Uno Family Trust and SC Credit Fund</td>\n",
       "      <td></td>\n",
       "      <td>23,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>HungerBox</td>\n",
       "      <td>FoodTech</td>\n",
       "      <td>Online Food Delivery Service</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>One97, Sabre Partners Trust, Pratithi Investme...</td>\n",
       "      <td>Series D1</td>\n",
       "      <td>1,560,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04/08/2020</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>Hyper-local Logistics</td>\n",
       "      <td>Online Delivery Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Existing Backers</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11/08/2020</td>\n",
       "      <td>Terra.do</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Climate School, E-learning</td>\n",
       "      <td>Stanford, California,</td>\n",
       "      <td>Stanford Angels and Entrepreneurs (India), BEE...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12/08/2020</td>\n",
       "      <td>Classplus</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>E-learning, Online Tutoring</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Falcon Edge</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>upto 15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14/08/2020</td>\n",
       "      <td>Niyo</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Niyo Solutions Inc.</td>\n",
       "      <td></td>\n",
       "      <td>6,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10/08/2020</td>\n",
       "      <td>ZestMoney</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Primrose Hills Ventures</td>\n",
       "      <td></td>\n",
       "      <td>10,670,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>07/08/2020</td>\n",
       "      <td>FreshToHome</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Food Delivery</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Ascent Capital</td>\n",
       "      <td>Venture</td>\n",
       "      <td>16,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>Eduvanz</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Sequoia India, Unitus</td>\n",
       "      <td>Series A</td>\n",
       "      <td>5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>08/09/2020</td>\n",
       "      <td>Byju’s</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Silver Lake, Tiger Global, General Atlantic an...</td>\n",
       "      <td>Private Equity</td>\n",
       "      <td>500,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12/09/2020</td>\n",
       "      <td>mCaffeine</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Skincare &amp; Haircare</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Amicus Capital Private Equity I LLP, Amicus Ca...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>3,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>09/09/2020</td>\n",
       "      <td>Qshala</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Curiosity Platform for Kids</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Rainmatter Capital</td>\n",
       "      <td>Angel</td>\n",
       "      <td>370,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>02/09/2020</td>\n",
       "      <td>Winzo</td>\n",
       "      <td>Online Gaming</td>\n",
       "      <td>Online Gaming</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Kalaari Capital Partners, IndigoEdge Managemen...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>15,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>09/09/2020</td>\n",
       "      <td>Hippo Video</td>\n",
       "      <td>Video Customer Experience(CX) Platform</td>\n",
       "      <td>Video Customer Experience(CX) Platform</td>\n",
       "      <td>Newark, Delaware, United States of Amercia</td>\n",
       "      <td>Alpha Wave Incubation, Exfinity Venture Partne...</td>\n",
       "      <td>Series A</td>\n",
       "      <td>4,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>07/09/2020</td>\n",
       "      <td>Melorra</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Jewelry Store</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Shadow Holdings, Lightbox.</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>upto 8,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>07/09/2020</td>\n",
       "      <td>1mg</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Pharmacy</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Gaja Capital, Tata Capital, Partners Group</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31/08/2020</td>\n",
       "      <td>mfine</td>\n",
       "      <td>HealthTech</td>\n",
       "      <td>On-Demand Healthcare Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Caretech Pte Inc</td>\n",
       "      <td>Series B</td>\n",
       "      <td>5,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31/08/2020</td>\n",
       "      <td>Apna</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>Recruitment Platform</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Lightspeed India and Sequoia Capital India</td>\n",
       "      <td>Series A</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>Railofy</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>WL &amp; RAC protection platform</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Chiratae Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>950,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date               Startup Name  \\\n",
       "0   15/07/2020                   Flipkart   \n",
       "1   16/07/2020                    Vedantu   \n",
       "2   16/07/2020                       Crio   \n",
       "3   14/07/2020                    goDutch   \n",
       "4   13/07/2020                   Mystifly   \n",
       "5   09/07/2020               JetSynthesys   \n",
       "6   10/07/2020                   gigIndia   \n",
       "7   15/07/2020                  PumPumPum   \n",
       "8   14/07/2020                       FLYX   \n",
       "9   13/07/2020  Open Appliances Pvt. Ltd.   \n",
       "10  15/08/2020                     Practo   \n",
       "11  13/08/2020                    Medlife   \n",
       "12  13/08/2020                  HungerBox   \n",
       "13  04/08/2020                      Dunzo   \n",
       "14  11/08/2020                   Terra.do   \n",
       "15  12/08/2020                  Classplus   \n",
       "16  14/08/2020                       Niyo   \n",
       "17  10/08/2020                  ZestMoney   \n",
       "18  07/08/2020                FreshToHome   \n",
       "19  13/08/2020                    Eduvanz   \n",
       "20  08/09/2020                     Byju’s   \n",
       "21  12/09/2020                  mCaffeine   \n",
       "22  09/09/2020                     Qshala   \n",
       "23  02/09/2020                      Winzo   \n",
       "24  09/09/2020                Hippo Video   \n",
       "25  07/09/2020                    Melorra   \n",
       "26  07/09/2020                        1mg   \n",
       "27  31/08/2020                      mfine   \n",
       "28  31/08/2020                       Apna   \n",
       "29  03/09/2020                    Railofy   \n",
       "\n",
       "                         Industry/Vertical  \\\n",
       "0                               E-commerce   \n",
       "1                                  EduTech   \n",
       "2                                  EduTech   \n",
       "3                                  FinTech   \n",
       "4                      Airfare Marketplace   \n",
       "5                 Gaming and Entertainment   \n",
       "6                              Marketplace   \n",
       "7                        Automotive Rental   \n",
       "8                               OTT Player   \n",
       "9                   Information Technology   \n",
       "10                              HealthTech   \n",
       "11                              E-commerce   \n",
       "12                                FoodTech   \n",
       "13                   Hyper-local Logistics   \n",
       "14                                 EduTech   \n",
       "15                                 EduTech   \n",
       "16                                 FinTech   \n",
       "17                                 FinTech   \n",
       "18                              E-commerce   \n",
       "19                                 FinTech   \n",
       "20                                 EduTech   \n",
       "21                           Personal Care   \n",
       "22                                 EduTech   \n",
       "23                           Online Gaming   \n",
       "24  Video Customer Experience(CX) Platform   \n",
       "25                              E-commerce   \n",
       "26                              E-commerce   \n",
       "27                              HealthTech   \n",
       "28                         Human Resources   \n",
       "29                          Transportation   \n",
       "\n",
       "                                         Sub-Vertical  \\\n",
       "0                                          E-commerce   \n",
       "1                                     Online Tutoring   \n",
       "2                    Learning Platform for Developers   \n",
       "3                                      Group Payments   \n",
       "4   Ticketing, Airline Retailing, and Post-Ticketi...   \n",
       "5                            Gaming and Entertainment   \n",
       "6                           Crowd Sourcing, Freelance   \n",
       "7                           Used Car-leasing platform   \n",
       "8                            Streaming Social Network   \n",
       "9               Internet-of-Things Security Solutions   \n",
       "10                           Health care and Wellness   \n",
       "11                                    Online Pharmacy   \n",
       "12                       Online Food Delivery Service   \n",
       "13                           Online Delivery Services   \n",
       "14                  Online Climate School, E-learning   \n",
       "15                        E-learning, Online Tutoring   \n",
       "16                                 Financial Services   \n",
       "17                                 Financial Services   \n",
       "18                                      Food Delivery   \n",
       "19                                 Financial Services   \n",
       "20                                    Online Tutoring   \n",
       "21                                Skincare & Haircare   \n",
       "22                 Online Curiosity Platform for Kids   \n",
       "23                                      Online Gaming   \n",
       "24             Video Customer Experience(CX) Platform   \n",
       "25                               Online Jewelry Store   \n",
       "26                                    Online Pharmacy   \n",
       "27                      On-Demand Healthcare Services   \n",
       "28                               Recruitment Platform   \n",
       "29                       WL & RAC protection platform   \n",
       "\n",
       "                                      Location  \\\n",
       "0                                    Bangalore   \n",
       "1                                    Bangalore   \n",
       "2                                    Bangalore   \n",
       "3                                       Mumbai   \n",
       "4                      Singapore and Bangalore   \n",
       "5                                         Pune   \n",
       "6                                         Pune   \n",
       "7                                      Gurgaon   \n",
       "8                           New York and Delhi   \n",
       "9                                    Bangalore   \n",
       "10                                   Bangalore   \n",
       "11                                   Bangalore   \n",
       "12                                   Bangalore   \n",
       "13                                   Bangalore   \n",
       "14                       Stanford, California,   \n",
       "15                                       Noida   \n",
       "16                                   Bangalore   \n",
       "17                                   Bangalore   \n",
       "18                                   Bangalore   \n",
       "19                                      Mumbai   \n",
       "20                                   Bangalore   \n",
       "21                                      Mumbai   \n",
       "22                                   Bangalore   \n",
       "23                                   New Delhi   \n",
       "24  Newark, Delaware, United States of Amercia   \n",
       "25                                   Bangalore   \n",
       "26                                     Gurgaon   \n",
       "27                                   Bangalore   \n",
       "28                                   Bangalore   \n",
       "29                                      Mumbai   \n",
       "\n",
       "                                             Investor         Investment Type  \\\n",
       "0                                         Walmart Inc                     M&A   \n",
       "1                                   Coatue Management                Series D   \n",
       "2                                         021 Capital            pre-Series A   \n",
       "3   Matrix India,Y Combinator, Global Founders Cap...                    Seed   \n",
       "4                                    Recruit Co. Ltd.            pre-Series B   \n",
       "5            Adar Poonawalla and Kris Gopalakrishnan.  Venture-Series Unknown   \n",
       "6        Incubate Fund India and Beyond Next Ventures            pre-Series A   \n",
       "7                            Early Adapters Syndicate                    Seed   \n",
       "8               Raj Mishra, founder of AIT Global Inc                pre-Seed   \n",
       "9                              Unicorn India Ventures  Venture-Series Unknown   \n",
       "10                                        A1A Company                Series F   \n",
       "11         Prasid Uno Family Trust and SC Credit Fund                           \n",
       "12  One97, Sabre Partners Trust, Pratithi Investme...               Series D1   \n",
       "13                                   Existing Backers             In Progress   \n",
       "14  Stanford Angels and Entrepreneurs (India), BEE...                    Seed   \n",
       "15                                        Falcon Edge             In Progress   \n",
       "16                                Niyo Solutions Inc.                           \n",
       "17                            Primrose Hills Ventures                           \n",
       "18                                     Ascent Capital                 Venture   \n",
       "19                              Sequoia India, Unitus                Series A   \n",
       "20  Silver Lake, Tiger Global, General Atlantic an...          Private Equity   \n",
       "21  Amicus Capital Private Equity I LLP, Amicus Ca...                Series B   \n",
       "22                                 Rainmatter Capital                   Angel   \n",
       "23  Kalaari Capital Partners, IndigoEdge Managemen...                Series B   \n",
       "24  Alpha Wave Incubation, Exfinity Venture Partne...                Series A   \n",
       "25                         Shadow Holdings, Lightbox.          Debt Financing   \n",
       "26         Gaja Capital, Tata Capital, Partners Group             In Progress   \n",
       "27                                   Caretech Pte Inc                Series B   \n",
       "28         Lightspeed India and Sequoia Capital India                Series A   \n",
       "29                                  Chiratae Ventures                    Seed   \n",
       "\n",
       "     Amount(in USD)  \n",
       "0     1,200,000,000  \n",
       "1       100,000,000  \n",
       "2           934,160  \n",
       "3         1,700,000  \n",
       "4         3,300,000  \n",
       "5           400,000  \n",
       "6           974,200  \n",
       "7           292,800  \n",
       "8           200,000  \n",
       "9           500,000  \n",
       "10       32,000,000  \n",
       "11       23,000,000  \n",
       "12        1,560,000  \n",
       "13       30,000,000  \n",
       "14        1,400,000  \n",
       "15  upto 15,000,000  \n",
       "16        6,000,000  \n",
       "17       10,670,000  \n",
       "18       16,200,000  \n",
       "19        5,000,000  \n",
       "20      500,000,000  \n",
       "21        3,000,000  \n",
       "22          370,000  \n",
       "23       15,500,000  \n",
       "24        4,500,000  \n",
       "25   upto 8,900,000  \n",
       "26      100,000,000  \n",
       "27        5,400,000  \n",
       "28        8,000,000  \n",
       "29          950,000  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Edge(r\"msedgedriver.exe\")\n",
    "time.sleep(2)\n",
    "\n",
    "# opening trak.in\n",
    "url = \"https://trak.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# do click on funding deals\n",
    "button = driver.find_element(By.XPATH,'//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(button)\n",
    "\n",
    "# Empty lists\n",
    "fund_dict = {}\n",
    "fund_dict['Date'] = []\n",
    "fund_dict['Startup Name'] = []\n",
    "fund_dict['Industry/Vertical'] = []\n",
    "fund_dict['Sub-Vertical'] = []\n",
    "fund_dict['Location'] = []\n",
    "fund_dict['Investor'] = []\n",
    "fund_dict['Investment Type'] = []\n",
    "fund_dict['Amount(in USD)'] = []\n",
    "\n",
    "for i in range(48,51):\n",
    "    # Date\n",
    "    dt = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "    \n",
    "    # Sub-Vertical\n",
    "    sv = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "    \n",
    "    # Investor\n",
    "    inv = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    # Amount\n",
    "    amt = driver.find_elements(By.XPATH,'//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "fund_df = pd.DataFrame(fund_dict)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Edge(r\"msedgedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url digit.in\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for best laptop\n",
    "best_gam_laptops = driver.find_element(By.XPATH,\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "Laptop_Name = []\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Weight = []\n",
    "Dimensions = []\n",
    "Graph_proc = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the data of laptop names\n",
    "laptop_name= driver.find_elements(By.XPATH,\"//div[@class='right-container']/div/a/h3\")\n",
    "for name in laptop_name:\n",
    "    Laptop_Name.append(name.text)\n",
    "    \n",
    "    \n",
    "#Scraping the data of operating system\n",
    "try:\n",
    "    op_sys = driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "    for os in op_sys:\n",
    "        Operating_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of display of the laptop\n",
    "try:\n",
    "    display= driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "    for disp in display:\n",
    "        Display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of Processor\n",
    "try:\n",
    "    processor = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[5]/td[3]\")\n",
    "    for pro in processor:\n",
    "        Processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "#Scraping data of memory\n",
    "try:\n",
    "    memory = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for memo in memory:\n",
    "        Memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of weight\n",
    "try:\n",
    "    weight = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "    for wgt in weight:\n",
    "        Weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of dimensions\n",
    "try:\n",
    "    dimension = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\")\n",
    "    for dim in dimension:\n",
    "        Dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of Graph processor\n",
    "try:\n",
    "    graph = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\")\n",
    "    for gra in graph:\n",
    "        Graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "#Scraping data of price\n",
    "try:\n",
    "    price = driver.find_elements(By.XPATH,\"//td[@class='smprice']\")\n",
    "    for pri in price:\n",
    "        Price.append(pri.text.replace('₹','Rs '))\n",
    "except NoSuchElementException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10 10 0 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Laptop_Name),len(Operating_sys),len(Display),len(Processor),len(Memory),len(Weight),len(Dimensions),len(Graph_proc),len(Price))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Name</th>\n",
       "      <th>Operating system</th>\n",
       "      <th>Display</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSI TITAN GT77-12UHS</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>17.3\" (3840 X 2160) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>64 GB DDR5 RAM &amp; 2 TB SSD</td>\n",
       "      <td>16 GB DDR6 NVIDIA GeForce RTX 3080 Ti Graphics...</td>\n",
       "      <td>397 x 330 x 23 mm dimension &amp; 3.3 kg weight</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALIENWARE X17 R2</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>17.3\" (1920X1080) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>32 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>16 GB DDR6 NVIDIA GeForce RTX 3080 Ti, Graphic...</td>\n",
       "      <td>399.23 x 299.57 x 14.75 mm dimension &amp; 3.02 kg...</td>\n",
       "      <td>Rs  389,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACER PREDATOR TRITON 500 SE PT516-52S</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16\" (2560 X 1600) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>32 GB DDR5 RAM &amp; 2 TB SSD</td>\n",
       "      <td>8 GB DDR6 NVIDIA GeForce RTXTM 3070 Ti Graphic...</td>\n",
       "      <td>358 x 262 x 19.9 mm dimension &amp; 2.4 kg weight</td>\n",
       "      <td>Rs  300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OMEN BY HP (16-B1371TX)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16.1\" (2560 X 1440) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>8 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>8 GB GDDR6 NVIDIA GeForce RTX 3070 Graphics card</td>\n",
       "      <td>369 x 248 x 23 mm dimension &amp; 2.32 kg weight</td>\n",
       "      <td>Rs  194,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACER PREDATOR HELIOS 300 AN515-45 (NH.QBRSI.0</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6\" (2560 X 1440) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 512 GB SSD</td>\n",
       "      <td>8 GB DDR6 NVIDIA GeForce RTX 3070 Graphics card</td>\n",
       "      <td>363 x 255 x 23.9 mm dimension &amp; 2.4 kg weight</td>\n",
       "      <td>Rs  172,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MSI DELTA 15 (A5EFK-083IN)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6\" (1920 X 1080) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB SSD</td>\n",
       "      <td>10 GB DDR6 AMD Radeon RX 6700M Graphics card</td>\n",
       "      <td>357 x 247 x 19 mm dimension &amp; 1.9 kg weight</td>\n",
       "      <td>Rs  188,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OMEN BY HP (16-C0141AX)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16.1\" (2560 X 1440) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB NVMe</td>\n",
       "      <td>8 GB GDDR6 AMD Radeon™ RX 6600M Graphics card</td>\n",
       "      <td>36.92 x 24.8 x 2.3 mm dimension &amp; 2.3 kg weight</td>\n",
       "      <td>Rs  129,899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LENOVO LEGION 5I PRO (82RF00MGIN)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>16\" (2560 X 1600) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR5 RAM &amp; 1 TB SSD</td>\n",
       "      <td>6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card</td>\n",
       "      <td>359.9 x 264.4 x 19.9 mm dimension &amp; 2.5 kg weight</td>\n",
       "      <td>Rs  230,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALIENWARE M15 R5 RYZEN EDITION ICC-C780001WIN</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6\" (1920 X 1080) DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 512 GB SSD</td>\n",
       "      <td>6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card</td>\n",
       "      <td>356.2 x 272.5 x 22.85 mm dimension &amp; 2.69 kg w...</td>\n",
       "      <td>Rs  144,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LENOVO SLIM 7 GEN 6 (82K8002JIN)</td>\n",
       "      <td>WINDOWS 11 HOME OS</td>\n",
       "      <td>15.6 MP | NA DISPLAY</td>\n",
       "      <td>Windows 11 Home</td>\n",
       "      <td>16 GB DDR4 RAM &amp; 1 TB SSD</td>\n",
       "      <td>6 GB DDR6 NVIDIA GeForce 3060 Max-Q Graphics card</td>\n",
       "      <td>356 x 252 x 16 mm dimension &amp; 1.9 kg weight</td>\n",
       "      <td>Rs  131,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Laptop Name    Operating system  \\\n",
       "0                           MSI TITAN GT77-12UHS  WINDOWS 11 HOME OS   \n",
       "1                               ALIENWARE X17 R2  WINDOWS 11 HOME OS   \n",
       "2          ACER PREDATOR TRITON 500 SE PT516-52S  WINDOWS 11 HOME OS   \n",
       "3                        OMEN BY HP (16-B1371TX)  WINDOWS 11 HOME OS   \n",
       "4  ACER PREDATOR HELIOS 300 AN515-45 (NH.QBRSI.0  WINDOWS 11 HOME OS   \n",
       "5                     MSI DELTA 15 (A5EFK-083IN)  WINDOWS 11 HOME OS   \n",
       "6                        OMEN BY HP (16-C0141AX)  WINDOWS 11 HOME OS   \n",
       "7              LENOVO LEGION 5I PRO (82RF00MGIN)  WINDOWS 11 HOME OS   \n",
       "8  ALIENWARE M15 R5 RYZEN EDITION ICC-C780001WIN  WINDOWS 11 HOME OS   \n",
       "9               LENOVO SLIM 7 GEN 6 (82K8002JIN)  WINDOWS 11 HOME OS   \n",
       "\n",
       "                       Display        Processor                       Memory  \\\n",
       "0  17.3\" (3840 X 2160) DISPLAY  Windows 11 Home    64 GB DDR5 RAM & 2 TB SSD   \n",
       "1    17.3\" (1920X1080) DISPLAY  Windows 11 Home    32 GB DDR5 RAM & 1 TB SSD   \n",
       "2    16\" (2560 X 1600) DISPLAY  Windows 11 Home    32 GB DDR5 RAM & 2 TB SSD   \n",
       "3  16.1\" (2560 X 1440) DISPLAY  Windows 11 Home     8 GB DDR5 RAM & 1 TB SSD   \n",
       "4  15.6\" (2560 X 1440) DISPLAY  Windows 11 Home  16 GB DDR4 RAM & 512 GB SSD   \n",
       "5  15.6\" (1920 X 1080) DISPLAY  Windows 11 Home    16 GB DDR4 RAM & 1 TB SSD   \n",
       "6  16.1\" (2560 X 1440) DISPLAY  Windows 11 Home   16 GB DDR4 RAM & 1 TB NVMe   \n",
       "7    16\" (2560 X 1600) DISPLAY  Windows 11 Home    16 GB DDR5 RAM & 1 TB SSD   \n",
       "8  15.6\" (1920 X 1080) DISPLAY  Windows 11 Home  16 GB DDR4 RAM & 512 GB SSD   \n",
       "9         15.6 MP | NA DISPLAY  Windows 11 Home    16 GB DDR4 RAM & 1 TB SSD   \n",
       "\n",
       "                                              Weight  \\\n",
       "0  16 GB DDR6 NVIDIA GeForce RTX 3080 Ti Graphics...   \n",
       "1  16 GB DDR6 NVIDIA GeForce RTX 3080 Ti, Graphic...   \n",
       "2  8 GB DDR6 NVIDIA GeForce RTXTM 3070 Ti Graphic...   \n",
       "3   8 GB GDDR6 NVIDIA GeForce RTX 3070 Graphics card   \n",
       "4    8 GB DDR6 NVIDIA GeForce RTX 3070 Graphics card   \n",
       "5       10 GB DDR6 AMD Radeon RX 6700M Graphics card   \n",
       "6      8 GB GDDR6 AMD Radeon™ RX 6600M Graphics card   \n",
       "7    6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card   \n",
       "8    6 GB DDR6 NVIDIA GeForce RTX 3060 Graphics card   \n",
       "9  6 GB DDR6 NVIDIA GeForce 3060 Max-Q Graphics card   \n",
       "\n",
       "                                          Dimensions        Price  \n",
       "0        397 x 330 x 23 mm dimension & 3.3 kg weight          N/A  \n",
       "1  399.23 x 299.57 x 14.75 mm dimension & 3.02 kg...  Rs  389,990  \n",
       "2      358 x 262 x 19.9 mm dimension & 2.4 kg weight  Rs  300,000  \n",
       "3       369 x 248 x 23 mm dimension & 2.32 kg weight  Rs  194,000  \n",
       "4      363 x 255 x 23.9 mm dimension & 2.4 kg weight  Rs  172,999  \n",
       "5        357 x 247 x 19 mm dimension & 1.9 kg weight  Rs  188,990  \n",
       "6    36.92 x 24.8 x 2.3 mm dimension & 2.3 kg weight  Rs  129,899  \n",
       "7  359.9 x 264.4 x 19.9 mm dimension & 2.5 kg weight  Rs  230,890  \n",
       "8  356.2 x 272.5 x 22.85 mm dimension & 2.69 kg w...  Rs  144,990  \n",
       "9        356 x 252 x 16 mm dimension & 1.9 kg weight  Rs  131,990  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  DataFrame for scraped data\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "Gaming_Laptop['Laptop Name'] = Laptop_Name\n",
    "Gaming_Laptop['Operating system'] = Operating_sys\n",
    "Gaming_Laptop['Display'] = Display\n",
    "Gaming_Laptop['Processor'] = Processor\n",
    "Gaming_Laptop['Memory'] = Memory\n",
    "Gaming_Laptop['Weight'] = Weight\n",
    "Gaming_Laptop['Dimensions'] = Dimensions\n",
    "\n",
    "Gaming_Laptop['Price'] = Price\n",
    "Gaming_Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataset to csv\n",
    "Gaming_Laptop.to_csv(\"GamingLaptops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8- Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Edge(r\"msedgedriver.exe\")\n",
    "time.sleep(2)\n",
    "#Getting the specified url\n",
    "url = \"https://www.forbes.com/?sh=69e6b8c92254\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the explore button\n",
    "button = driver.find_element(By.XPATH,'//div[@class=\"_69hVhdY4\"]')\n",
    "button.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select billionaire  \n",
    "bill = driver.find_element(By.XPATH,'//li[@class=\"TjJgrPSg cD45ib6e primary\"]')\n",
    "bill.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select world billionaire  \n",
    "world_bill= driver.find_element(By.XPATH,'//li[@class=\"TjJgrPSg _2bNo56RE secondary\"]')\n",
    "world_bill.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping required data from the web page\n",
    "#Creating empty lists\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "total_net_worth = []\n",
    "Age = []\n",
    "citizenship = []\n",
    "Source = []\n",
    "industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    #Scraping the data of rank of the billionaires\n",
    "    rank_tags= driver.find_elements(By.XPATH,\"//div[@class='rank']\")\n",
    "    for rank in rank_tags:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping the data of names of the billionaires\n",
    "    name_tags= driver.find_elements(By.XPATH,\"//div[@class='personName']/div\")\n",
    "    for name in name_tags:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of age of the billionaires\n",
    "    age_tags= driver.find_elements(By.XPATH,\"//div[@class='age']/div\")\n",
    "    for age in age_tags:\n",
    "        Age.append(age.text)   \n",
    "    time.sleep(1)\n",
    "    \n",
    "    #Scraping data of citizenship of the billionaires\n",
    "    cit_tags= driver.find_elements(By.XPATH,\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tags:\n",
    "        citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of source of income of the billionaires\n",
    "    sour_tags= driver.find_elements(By.XPATH,\"//div[@class='source']\")\n",
    "    for sour in sour_tags:\n",
    "        Source.append(sour.text)    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of Industry of the billionaires\n",
    "    ind_tags= driver.find_elements(By.XPATH,\"//div[@class='category']//div\")\n",
    "    for ind in ind_tags:\n",
    "        industry.append(ind.text)\n",
    "        \n",
    "        \n",
    "    #scraping data of net_worth of billionaires\n",
    "    net_tags= driver.find_elements(By.XPATH,\"//div[@class='netWorth']//div[1]\")\n",
    "    for net in net_tags:\n",
    "        total_net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #Clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "#Scraping data of net worth\n",
    "Net_Worth = []\n",
    "for i in range(0,len(total_net_worth),2):\n",
    "    Net_Worth.append(total_net_worth[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Names</th>\n",
       "      <th>Net Worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$219 B</td>\n",
       "      <td></td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$171 B</td>\n",
       "      <td></td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$158 B</td>\n",
       "      <td></td>\n",
       "      <td>France</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$129 B</td>\n",
       "      <td></td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Warren Buffett</td>\n",
       "      <td>$118 B</td>\n",
       "      <td></td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                     Names Net Worth Age    Citizenship Source  \\\n",
       "0     1.                 Elon Musk    $219 B      United States          \n",
       "1     2.                Jeff Bezos    $171 B      United States          \n",
       "2     3.  Bernard Arnault & family    $158 B             France          \n",
       "3     4.                Bill Gates    $129 B      United States          \n",
       "4     5.            Warren Buffett    $118 B      United States          \n",
       "..   ...                       ...       ...  ..            ...    ...   \n",
       "195                                                                      \n",
       "196                                                                      \n",
       "197                                                                      \n",
       "198                                                                      \n",
       "199                                                                      \n",
       "\n",
       "    Industry  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "195           \n",
       "196           \n",
       "197           \n",
       "198           \n",
       "199           \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Framing data\n",
    "Billionaires=pd.DataFrame({})\n",
    "Billionaires['Rank'] = Rank\n",
    "Billionaires['Names'] = Person_Name\n",
    "Billionaires['Net Worth'] = Net_Worth\n",
    "Billionaires['Age'] = Age\n",
    "Billionaires['Citizenship'] = citizenship\n",
    "Billionaires['Source'] = Source\n",
    "Billionaires['Industry'] = industry\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "driver.get(\"https://www.youtube.com/\")\n",
    "search_tag=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/form/div[1]/div[1]/input')\n",
    "search_tag.send_keys('Peaches')\n",
    "search=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/button')\n",
    "search.click() \n",
    "time.sleep(3)\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Peaches_video=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-video-renderer[1]/div[1]/div/div[1]/div/h3/a')\n",
    "Peaches_video.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments=[]\n",
    "time_posteds=[]\n",
    "Likes=[]\n",
    "No_of_Likes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 time we scroll down by 10000 in order to generate more Comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment=driver.find_elements(By.ID,\"content-text\")\n",
    "for i in comment[0:501]:\n",
    "       comments.append(i.text)\n",
    "time_posted=driver.find_elements(By.XPATH,'//yt-formatted-string[@class=\"published-time-text style-scope ytd-comment-renderer\"]')\n",
    "for i in time_posted[0:501]:\n",
    "       time_posteds.append(i.text)\n",
    "like = driver.find_elements(By.XPATH,\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2)[0:501]:\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 40\n"
     ]
    }
   ],
   "source": [
    "print(len(comments),len(time_posteds),len(No_of_Likes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Time_Posted</th>\n",
       "      <th>No_of_Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUMMER VIBES  Don't Forget to Eat Your Fruits</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>10K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kaim always  but my favourite is lemonade &amp; Ca...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfect Mixture of Western and Punjabi Music. ...</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your songs just keep getting better and better...</td>\n",
       "      <td>11 days ago</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legendary Punjabi singer I had seen ever</td>\n",
       "      <td>8 days ago</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Diljit Paaji Has Passed The Tag Of A Artist......</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beats will remain in my head till winters atea...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Diljit Dosanjh never disappoint. New  summer v...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>He Is Always trying Something Different Nd Alw...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I love the vibe of this song. It hits different</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Noone can make music like punjabi people</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dude opening me up to a new spectrum of music,...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>On repeat, in my research lab. Like the peache...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hands down to DILJIT,RAJ,INTENSE all 3 of them...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>There is a large number of creater in Punjabi ...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Diljit Dosanjh never disappoint. New summer vi...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This man will make history one day</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>the vibe is awsm and the lyrics , flow everyth...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lyrics are always lit..\\nDiljit भाई ️</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>At this point, even if diljit sings a lullaby ...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This man is on another level than any other Pu...</td>\n",
       "      <td>11 days ago</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>This guy constantly coming up with HITS ..Amaz...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>This man has continuosly gave us a song to vibe</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>No one can match him. When you knows what exac...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"When someone support someone from start and t...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>️️\"One of the best thing about dude is that he...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>those two bright eyes, that killer smile, that...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>One of the best thing about dude is that he ne...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Diljit Dosanjh is a very good person️&amp;mood cha...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>INTENSE never fails to impress!  Sickkkk beat!</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Hey, just wanted to say I’m proud how far the ...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>️\"One of the best thing about dude is that he ...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>This guy constantly coming up with HITS ..Amaz...</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>His songs really give me great vibes and this ...</td>\n",
       "      <td>1 month ago (edited)</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>On repeat …music lyrics and Diljit made a blast</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>One thing that i have never seen or heard befo...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Intense never disappoints ️️</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Fell for this song ...the first time I heard i...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Hits after hits  Another banger</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>When you support someone from start and than u...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comments           Time_Posted  \\\n",
       "0       SUMMER VIBES  Don't Forget to Eat Your Fruits           1 month ago   \n",
       "1   kaim always  but my favourite is lemonade & Ca...           1 month ago   \n",
       "2   Perfect Mixture of Western and Punjabi Music. ...           3 weeks ago   \n",
       "3   Your songs just keep getting better and better...           11 days ago   \n",
       "4           Legendary Punjabi singer I had seen ever             8 days ago   \n",
       "5   Diljit Paaji Has Passed The Tag Of A Artist......           1 month ago   \n",
       "6   Beats will remain in my head till winters atea...           1 month ago   \n",
       "7   Diljit Dosanjh never disappoint. New  summer v...           1 month ago   \n",
       "8   He Is Always trying Something Different Nd Alw...           1 month ago   \n",
       "9    I love the vibe of this song. It hits different            1 month ago   \n",
       "10          Noone can make music like punjabi people             5 days ago   \n",
       "11  Dude opening me up to a new spectrum of music,...           1 month ago   \n",
       "12  On repeat, in my research lab. Like the peache...           1 month ago   \n",
       "13  Hands down to DILJIT,RAJ,INTENSE all 3 of them...           1 month ago   \n",
       "14  There is a large number of creater in Punjabi ...           1 month ago   \n",
       "15  Diljit Dosanjh never disappoint. New summer vi...           1 month ago   \n",
       "16                 This man will make history one day           2 weeks ago   \n",
       "17  the vibe is awsm and the lyrics , flow everyth...           1 month ago   \n",
       "18              Lyrics are always lit..\\nDiljit भाई ️           1 month ago   \n",
       "19  At this point, even if diljit sings a lullaby ...           1 month ago   \n",
       "20  This man is on another level than any other Pu...           11 days ago   \n",
       "21  This guy constantly coming up with HITS ..Amaz...           1 month ago   \n",
       "22    This man has continuosly gave us a song to vibe           2 weeks ago   \n",
       "23  No one can match him. When you knows what exac...           1 month ago   \n",
       "24  \"When someone support someone from start and t...           1 month ago   \n",
       "25  ️️\"One of the best thing about dude is that he...           1 month ago   \n",
       "26  those two bright eyes, that killer smile, that...           1 month ago   \n",
       "27  One of the best thing about dude is that he ne...           1 month ago   \n",
       "28  Diljit Dosanjh is a very good person️&mood cha...           2 weeks ago   \n",
       "29     INTENSE never fails to impress!  Sickkkk beat!           1 month ago   \n",
       "30  Hey, just wanted to say I’m proud how far the ...           1 month ago   \n",
       "31  ️\"One of the best thing about dude is that he ...           1 month ago   \n",
       "32  This guy constantly coming up with HITS ..Amaz...           3 weeks ago   \n",
       "33  His songs really give me great vibes and this ...  1 month ago (edited)   \n",
       "34    On repeat …music lyrics and Diljit made a blast           1 month ago   \n",
       "35  One thing that i have never seen or heard befo...           1 month ago   \n",
       "36                       Intense never disappoints ️️           1 month ago   \n",
       "37  Fell for this song ...the first time I heard i...           1 month ago   \n",
       "38                   Hits after hits  Another banger            1 month ago   \n",
       "39  When you support someone from start and than u...           1 month ago   \n",
       "\n",
       "   No_of_Likes  \n",
       "0          10K  \n",
       "1          634  \n",
       "2          151  \n",
       "3           27  \n",
       "4           37  \n",
       "5          117  \n",
       "6           38  \n",
       "7          475  \n",
       "8          227  \n",
       "9           48  \n",
       "10           1  \n",
       "11          17  \n",
       "12          29  \n",
       "13         205  \n",
       "14         206  \n",
       "15         239  \n",
       "16          42  \n",
       "17          14  \n",
       "18          24  \n",
       "19         175  \n",
       "20          22  \n",
       "21          82  \n",
       "22          42  \n",
       "23         131  \n",
       "24          33  \n",
       "25          27  \n",
       "26          13  \n",
       "27          13  \n",
       "28          12  \n",
       "29          88  \n",
       "30          19  \n",
       "31          25  \n",
       "32          25  \n",
       "33          49  \n",
       "34           4  \n",
       "35         184  \n",
       "36           9  \n",
       "37          16  \n",
       "38          73  \n",
       "39          82  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Comments':comments,'Time_Posted':time_posteds,'No_of_Likes':No_of_Likes})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.hostelworld.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tag=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[1]/div/div/div[4]/div/div[2]/div/div[1]/div/input')\n",
    "london_tag.send_keys('London')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_btn=driver.find_element(By.XPATH,\"/html/body/div[3]/div/div/div[2]/div[1]/div/div/div[4]/div/div[2]/div/div[1]/div/div/div/span/i\")\n",
    "london_btn.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hostel_Name = []\n",
    "Distance = []\n",
    "overall_review = []\n",
    "total_reviews = []\n",
    "facilities = []\n",
    "price = []\n",
    "Rating = []\n",
    "property_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    \n",
    "    # Hostel name\n",
    "    names = driver.find_elements(By.XPATH,'//h2[@class=\"title title-6\"]')\n",
    "    for name in names:\n",
    "        Hostel_Name.append(name.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Distance from city centre\n",
    "    dis = driver.find_elements(By.XPATH,'//span[@class=\"description\"]')\n",
    "    for d in dis:\n",
    "        Distance.append(d.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Overall Review    \n",
    "    review = driver.find_elements(By.XPATH,'//div[@class=\"keyword\"]//span')\n",
    "    for r in review:\n",
    "        overall_review.append(r.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Total No of reviews     \n",
    "    t_review = driver.find_elements(By.XPATH,'//div[@class=\"reviews\"]')\n",
    "    for tr in t_review:\n",
    "        total_reviews.append(tr.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # facilities\n",
    "    service = driver.find_elements(By.XPATH,'//div[@class=\"facilities-label facilities\"]')\n",
    "    for s in service:\n",
    "        facilities.append(s.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Prices    \n",
    "    prices = driver.find_elements(By.XPATH,'//div[@class=\"price-col\"]')\n",
    "    for p in prices:\n",
    "        price.append(p.text)\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,'//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "time.sleep(4)        \n",
    "        \n",
    "# Separate  Privates_From price  and  Dorms_From price\n",
    "Privates_From = []\n",
    "for i in range(0,len(price),2):\n",
    "    Privates_From.append(price[i])\n",
    "time.sleep(2)\n",
    "\n",
    "Dorms_From = []\n",
    "for i in range(1,len(price),2):\n",
    "    Dorms_From.append(price[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape Hostels URL\n",
    "hostel_url = []\n",
    "\n",
    "while(True):\n",
    "    urls = driver.find_elements(By.XPATH,'//h2[@class=\"title title-6\"]//a')\n",
    "    for url in urls:\n",
    "        hostel_url.append(url.get_attribute(\"href\"))\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,'//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "\n",
    "        \n",
    "Rate = []\n",
    "for page in hostel_url:\n",
    "    driver.get(page)\n",
    "    \n",
    "    # Rating\n",
    "    try:\n",
    "        ratings = driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[1]/section/div[6]/div/div[1]/div[1]/div[1]')\n",
    "        Rate.append(ratings.text)\n",
    "    except NoSuchElementException:\n",
    "        Rate.append(\"No Rating\")  \n",
    "    time.sleep(2)\n",
    "    \n",
    "     # Property Description\n",
    "    try:\n",
    "        pd = driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[1]/section/div[6]/div/div[2]/div[2]/div/div[2]')\n",
    "        property_description.append(pd.text)\n",
    "    except NoSuchElementException:\n",
    "        property_description.append(\"No Description\")  \n",
    "\n",
    "    \n",
    "time.sleep(2)        \n",
    "# remove extra data from Rating     \n",
    "all_text = []\n",
    "for i in Rate:\n",
    "    all_text.append(i.split())\n",
    "time.sleep(2)\n",
    "\n",
    "for i in all_text:\n",
    "    Rating.append(i[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(Hostel_Name),len(Distance),len(total_reviews),len(facilities),len(Rating),len(property_description),len(Privates_From),len(Dorms_From))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Distance_city</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Tot_review</th>\n",
       "      <th>Pri_frm_price</th>\n",
       "      <th>Dorms_frm_price</th>\n",
       "      <th>Facility</th>\n",
       "      <th>Prop_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Distance_city, Rating, Tot_review, Pri_frm_price, Dorms_frm_price, Facility, Prop_desc]\n",
       "Index: []"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Name':Hostel_Name,'Distance_city':Distance,'Rating':Rating,'Tot_review':total_reviews,'Pri_frm_price':Privates_From,'Dorms_frm_price':Dorms_From,'Facility':facilities,'Prop_desc':property_description})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
